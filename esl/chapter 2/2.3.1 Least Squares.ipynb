{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares\n",
    "\n",
    "This is a python implementation of the linear least squares classification algorithms presented in Section 2.3.1.\n",
    "\n",
    "First, let's define the notations used in this example:\n",
    "   * $\\textbf{X}$ is a matrix of inputs with size $N \\times p$, where $N$ is the number of obervations and $p$ is the number of columns/ features / dimension.\n",
    "       * Note that \n",
    "       $$\\textbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\cdots & x_{1p}\\\\\n",
    "                                     x_{21} & x_{22} & \\cdots & x_{2p}\\\\\n",
    "                                     \\vdots & \\cdots & \\cdots & \\vdots\\\\\n",
    "                                     x_{N1} & \\cdots & \\cdots & x_{Np}\n",
    "                      \\end{bmatrix}\n",
    "       $$\n",
    "       where $x_{ij}$ denotes the $i^{th}$ observation (row) of the $j^{th}$ feature of $\\textbf{X}$ and $X_k$ denotes the $k^{th}$ column of $\\textbf{X}$.\n",
    "   * $\\textbf{Y}$ is a matrix of outputs with size $N \\times q$, where $q$ is the number of outputs. In this example, we have only one output.\n",
    "   * $\\beta = [\\beta_1, \\beta_2, \\cdots, \\beta_p]^\\top$ is a vector of parameters with size $p$.\n",
    "   * Any variables with a  $\\hat{}$ on top  denote predicted / estimated variables.\n",
    "   * $\\textbf{e}$ denotes a vector of ones with an appropriate size.\n",
    "\n",
    "### Linear model in matrix form\n",
    "Now, let's define our least squares problem.\n",
    "\n",
    "Given a vector of inputs $\\textbf{X}$, the linear model is constructed as:\n",
    "$$\\begin{equation}\\textbf{Y} = \\textbf{X}\\beta + \\textbf{e}\\beta_0 + \\epsilon\\end{equation}$$\n",
    "where $\\beta_0$ is the intercept/offset (bias in machine learning), $\\epsilon$ is a vector of unobserved disturbances, and $\\textbf{e}$ is a vector of ones with size $N$.\n",
    "It is convenient to absorb $\\beta_0$ into $\\beta$ such that:\n",
    "\n",
    "$$\\textbf{Y} = [\\textbf{e}, X_1, X_2, ... X_p] \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots\\\\ \\beta_p\\end{bmatrix} + \\epsilon = \\textbf{X}\\beta + \\epsilon$$\n",
    "\n",
    "### Residuals and the least squares criterion\n",
    "If $\\hat\\beta$ is a vector of estimates of the actual $\\beta$, then the prediction model can be written as\n",
    "$$\\textbf{Y} = \\textbf{X}\\hat\\beta + e$$\n",
    "where $e$ is a vector of residual or error between $\\textbf{Y}$ and $\\hat{\\textbf{Y}}$.\n",
    "\n",
    "The residual/error $e$ can be computed using the data and $\\hat\\beta$ by means of\n",
    "$$e=\\textbf{Y}-\\textbf{X}\\beta$$\n",
    "The most popular method to fit the linear model to the training data is *least squares*, which minimizes the residual sum of squares (RSS) of errors w.r.t to $\\beta$:\n",
    "\n",
    "$$\\begin{aligned}RSS(\\beta)&=\\sum_{i=1}^N e_i^2=\\sum_{i=1}^N (Y_i - X_{i, :}\\beta)^2\\\\ \n",
    "&= (\\textbf{Y} - \\textbf{X}\\beta)^\\top(\\textbf{Y} - \\textbf{X}\\beta)\\\\&=\\textbf{Y}^\\top\\textbf{Y}-\\textbf{Y}^\\top\\textbf{X}\\beta-\\beta^\\top\\textbf{X}^\\top\\textbf{Y} + \\beta^\\top\\textbf{X}^\\top\\textbf{X}\\beta\\end{aligned}$$\n",
    "To compute $\\beta$ that minimizes the $RSS(\\beta)$:\n",
    "$$\\frac{\\partial RSS(\\beta)}{\\partial\\beta} = -2\\textbf{X}^\\top\\textbf{Y}+2\\textbf{X}^\\top\\textbf{X}\\beta=0$$\n",
    "which gives the normal equation\n",
    "$$\\textbf{X}^\\top\\textbf{X}\\beta=\\textbf{X}^\\top\\textbf{Y}$$\n",
    "\n",
    "Solving this for $\\beta$, we obtain:\n",
    "$$\\beta = (\\textbf{X}^\\top\\textbf{X})^{-1}\\textbf{X}^\\top\\textbf{Y}$$\n",
    "\n",
    "provided that $(\\textbf{X}^\\top\\textbf{X})^{-1}$ exists, or in other words $\\textbf{X}$ should have rank $p$. In particular, this requires that $N\\geq p$ (number of observations is greater than or equal the number of parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python's implementation using Mixture dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from ml_datasets.esl import Mixture\n",
    "from ml_datasets.utils import plot_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "mixture = Mixture()\n",
    "x, y = mixture.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the input and output shapes\n",
    "print(\"x: {}, y: {}\".format(x.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "plt = plot_2D(x, y, \"ESL-Mixture Dataset\", axis='on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append intercept in the input x, so x = [1, x1, x2]\n",
    "x = np.concatenate((np.ones_like(x[:, 0].reshape(-1, 1)), x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution of the least square is: $\\hat\\beta = (\\textbf{X}^\\top \\textbf{X})^{-1} \\textbf{X}^\\top \\textbf{Y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if dot(x.T, x) is invertible\n",
    "if np.linalg.cond(np.dot(x.T, x)) < 1/sys.float_info.epsilon:\n",
    "    beta_hat = np.dot(np.dot(np.linalg.inv(np.dot(x.T, x)), x.T), y)\n",
    "else:\n",
    "    print(\"dot(x.T, x) is ill-conditioned\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated output is given as $\\hat{\\textbf{Y}} = \\textbf{X}\\hat\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.dot(x, beta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the final classification output is given as:\n",
    "\n",
    "$\\hat{\\textbf{G}} = \\begin{cases}\n",
    "    \\color{orange}{\\text{ORANGE}} \\color{black}{\\text{ or 1}}, & \\text{if } \\hat{\\textbf{Y}} > 0.5\\\\\n",
    "    \\color{blue}{\\text{BLUE}} \\color{black}{\\text{ or 0}},              & \\text{if } \\hat{\\textbf{Y}} \\leq 0.5\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "G_hat = np.array([1 if y_hat_ > threshold else 0 for y_hat_ in y_hat]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is given as $\\hat{\\textbf{Y}} = 0.5\\textbf{e} = \\textbf{X} \\hat\\beta =  \\beta_0\\textbf{e} + X_1\\beta_1 + X_2\\beta_2$, which implies $X_2 = -\\frac{\\hat\\beta_1}{\\hat\\beta_2} X_1+ \\frac{0.5\\textbf{e}}{\\hat\\beta_2} - \\frac{\\hat\\beta_0}{\\hat\\beta_2}, \\forall X_1 \\in \\mathcal{R}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the classification results and decision boundary\n",
    "BLUE, ORANGE = \"#57B5E8\", \"#E69E00\"\n",
    "x_1 = np.linspace(min(x[:, 1]) - 0.5, max(x[:, 1]) + 0.5,100)\n",
    "x_2 = - (beta_hat[1] / beta_hat[2]) * x_1 + (threshold / beta_hat[2]) - (beta_hat[0] / beta_hat[2])\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.scatter(x[:, 1][y==1], x[:, 2][y==1], s=100, marker=\"o\", edgecolors=ORANGE, facecolors=\"none\", label='ground truth - class 1')\n",
    "plt.scatter(x[:, 1][y==0], x[:, 2][y==0], s=100, marker=\"o\", edgecolors=BLUE, facecolors=\"none\", label='ground truth - class 0')\n",
    "plt.scatter(x[:, 1][G_hat==1] + 0.03, x[:, 2][G_hat==1], s=100, marker=\"*\", edgecolors=ORANGE, facecolors=\"none\", label='prediction - class 1')\n",
    "plt.scatter(x[:, 1][G_hat==0] + 0.03, x[:, 2][G_hat==0], s=100, marker=\"*\", edgecolors=BLUE, facecolors=\"none\", label='prediction - class 0')\n",
    "plt.plot(x_1, x_2, '--k', label='decision boundary')\n",
    "plt.legend()\n",
    "plt.title(\"Linear Least Squares Classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
